Running the experiments
The baseline experiment trains the model in the conventional way.

To run the baseline experiment with MNIST on MLP using CPU:
python src/baseline_main.py --model=mlp --dataset=mnist --epochs=10
Or to run it on GPU (eg: if gpu:0 is available):
python src/baseline_main.py --model=mlp --dataset=mnist --gpu=0 --epochs=10
Federated experiment involves training a global model using many local models.
To run the federated experiment with CIFAR on CNN (IID):
python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=10
To run the same experiment under non-IID condition:
python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=0 --epochs=10
You can change the default values of other parameters to simulate different conditions. Refer to the options section.

Options
The default values for various paramters parsed to the experiment are given in options.py. Details are given some of those parameters:

--dataset: Default: 'mnist'. Options: 'mnist', 'fmnist', 'cifar'
--model: Default: 'mlp'. Options: 'mlp', 'cnn'
--gpu: Default: None (runs on CPU). Can also be set to the specific gpu id.
--epochs: Number of rounds of training.
--lr: Learning rate set to 0.01 by default.
--verbose: Detailed log outputs. Activated by default, set to 0 to deactivate.
--seed: Random Seed. Default set to 1.
Federated Parameters
--iid: Distribution of data amongst users. Default set to IID. Set to 0 for non-IID.
--num_users:Number of users. Default is 100.
--frac: Fraction of users to be used for federated updates. Default is 0.1.
--local_ep: Number of local training epochs in each user. Default is 10.
--local_bs: Batch size of local updates in each user. Default is 10.
--unequal: Used in non-iid setting. Option to split the data amongst users equally or unequally. Default set to 0 for equal splits. Set to 1 for unequal splits.
